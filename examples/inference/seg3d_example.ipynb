{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D segmentation task with FLaVor inference service"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide will walk you through tailoring the FLaVor inference service for 3D segmentation tasks using the model from [Monai](https://monai.io/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the working environment, please ensure you have the following dependencies installed:\n",
    "\n",
    "```\n",
    "python >= 3.10\n",
    "torch >= 1.13\n",
    "monai >= 1.1.0 and monai[einops]\n",
    "numpy < 2.0.0\n",
    "```\n",
    "\n",
    "or simply run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry install --with seg3d_example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, download pretrain weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pwd: examples/inference\n",
    "!wget https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/swin_unetr.tiny_5000ep_f12_lr2e-4_pretrained.pt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, Callable, Dict, List, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import scipy.ndimage as ndimage\n",
    "import SimpleITK as sitk\n",
    "import torch\n",
    "from monai import transforms\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.networks.nets import SwinUNETR\n",
    "\n",
    "from flavor.serve.apps import InferAPP\n",
    "from flavor.serve.inference.data_models.api import (\n",
    "    AiCOCOImageInputDataModel,\n",
    "    AiCOCOImageOutputDataModel,\n",
    ")\n",
    "from flavor.serve.inference.data_models.functional import AiImage\n",
    "from flavor.serve.inference.inference_models import BaseAiCOCOImageInferenceModel\n",
    "from flavor.serve.inference.strategies import AiCOCOSegmentationOutputStrategy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup inference model\n",
    "\n",
    "In this section, we would create `ClassificationInferenceModel` inheriting from `BaseAiCOCOImageInferenceModel`. There are few abstract methods that we must override such as `define_inference_network`, `set_categories`, `set_regressions`, `data_reader` and `output_formatter`. For the inference process related methods such as `preprocess`, `inference` and `postprocess`, we override them if necessary. `preprocess` and `postprocess` would remain an identical operation if unmodified. `inference` by default runs `self.forward(x)`.\n",
    "\n",
    "Firstly, we need to implement submethods: `define_inference_network`, `set_categories` and `set_regressions`. These are defined in the `__init__()` constructor of the parent class `BaseAiCOCOImageInferenceModel`. `define_inference_network` defines your inference network and loads its pre-trained weight. `set_categories` and `set_regressions` define category and regression information. For example, a segmentation output would contain `c` channels. We need to show the exact meaning of each channel by specifying in `set_categories`. Refer to the following example for more detail.\n",
    "\n",
    "Next, we implement other submethods that would be used in the `__call__` function of our inference model. See below workflow."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `__call__` function workflow for the inference model\n",
    "![__call__](images/call.png \"inference workflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationInferenceModel(BaseAiCOCOImageInferenceModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.formatter = AiCOCOSegmentationOutputStrategy()\n",
    "\n",
    "    def define_inference_network(self) -> Callable:\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = SwinUNETR(\n",
    "            img_size=(96, 96, 96),\n",
    "            in_channels=1,\n",
    "            out_channels=14,\n",
    "            feature_size=12,\n",
    "            use_checkpoint=True,\n",
    "        )\n",
    "        state_dict = torch.hub.load_state_dict_from_url(\n",
    "            \"https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/swin_unetr.tiny_5000ep_f12_lr2e-4_pretrained.pt\",\n",
    "            progress=True,\n",
    "            map_location=self.device,\n",
    "        )[\"state_dict\"]\n",
    "\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.eval()\n",
    "        model.to(self.device)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def set_categories(self) -> List[Dict[str, Any]]:\n",
    "        categories = [\n",
    "            {\"name\": \"Background\", \"display\": False},\n",
    "            {\"name\": \"Spleen\", \"display\": True},\n",
    "            {\"name\": \"Right Kidney\", \"display\": True},\n",
    "            {\"name\": \"Left Kidney\", \"display\": True},\n",
    "            {\"name\": \"Gallbladder\", \"display\": True},\n",
    "            {\"name\": \"Esophagus\", \"display\": True},\n",
    "            {\"name\": \"Liver\", \"display\": True},\n",
    "            {\"name\": \"Stomach\", \"display\": True},\n",
    "            {\"name\": \"Aorta\", \"display\": True},\n",
    "            {\"name\": \"IVC\", \"display\": True},\n",
    "            {\"name\": \"Portal and Splenic Veins\", \"display\": True},\n",
    "            {\"name\": \"Pancreas\", \"display\": True},\n",
    "            {\"name\": \"Right adrenal gland\", \"display\": True},\n",
    "            {\"name\": \"Left adrenal gland\", \"display\": True},\n",
    "        ]\n",
    "        return categories\n",
    "\n",
    "    def set_regressions(self) -> None:\n",
    "        return None\n",
    "\n",
    "    def data_reader(\n",
    "        self, files: Sequence[str], **kwargs\n",
    "    ) -> Tuple[np.ndarray, List[str]]:\n",
    "        def sort_images_by_z_axis(filenames):\n",
    "\n",
    "            sorted_reader_filename_pairs = []\n",
    "\n",
    "            for f in filenames:\n",
    "                dicom_reader = sitk.ImageFileReader()\n",
    "                dicom_reader.SetFileName(f)\n",
    "                dicom_reader.ReadImageInformation()\n",
    "\n",
    "                sorted_reader_filename_pairs.append((dicom_reader, f))\n",
    "\n",
    "            zs = [\n",
    "                float(r.GetMetaData(key=\"0020|0032\").split(\"\\\\\")[-1])\n",
    "                for r, _ in sorted_reader_filename_pairs\n",
    "            ]\n",
    "\n",
    "            sort_inds = np.argsort(zs)\n",
    "            sorted_reader_filename_pairs = [sorted_reader_filename_pairs[s] for s in sort_inds]\n",
    "\n",
    "            return sorted_reader_filename_pairs\n",
    "\n",
    "        pairs = sort_images_by_z_axis(files)\n",
    "\n",
    "        readers, sorted_filenames = zip(*pairs)\n",
    "        sorted_filenames = list(sorted_filenames)\n",
    "\n",
    "        simages = [sitk.GetArrayFromImage(r.Execute()).squeeze() for r in readers]\n",
    "        volume = np.stack(simages)\n",
    "        volume = np.expand_dims(volume, axis=0)\n",
    "        \n",
    "        self.metadata = volume.shape[1:]\n",
    "\n",
    "        return volume, sorted_filenames \n",
    "\n",
    "    def preprocess(self, data: np.ndarray) -> torch.Tensor:\n",
    "        infer_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Spacing(pixdim=(1.5, 1.5, 2.0), mode=\"bilinear\"),\n",
    "                transforms.ScaleIntensityRange(\n",
    "                    a_min=175.0, a_max=250.0, b_min=0.0, b_max=1.0, clip=True\n",
    "                ),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "        data = infer_transform(data).unsqueeze(0).to(self.device)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def inference(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            out = sliding_window_inference(\n",
    "                x, (96, 96, 96), 4, self.network, overlap=0.5, mode=\"gaussian\"\n",
    "            )\n",
    "        return out\n",
    "\n",
    "    def postprocess(self, out: torch.Tensor) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply softmax and perform inverse resample back to original image size.\n",
    "\n",
    "        Args:\n",
    "            out (torch.Tensor): Inference model output.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Prediction output.\n",
    "        \"\"\"\n",
    "\n",
    "        def resample_3d(img, target_size):\n",
    "            imx, imy, imz = img.shape\n",
    "            tx, ty, tz = target_size\n",
    "            zoom_ratio = (float(tx) / float(imx), float(ty) / float(imy), float(tz) / float(imz))\n",
    "            img_resampled = ndimage.zoom(img, zoom_ratio, order=0, prefilter=False)\n",
    "            return img_resampled\n",
    "\n",
    "        c = out.shape[1]\n",
    "        output = torch.softmax(out, 1).cpu().numpy()\n",
    "        output = np.argmax(output, axis=1).astype(np.uint8)[0]\n",
    "\n",
    "        output = resample_3d(output, self.metadata)\n",
    "        binary_output = np.zeros([c] + list(output.shape))\n",
    "        for i in range(c):\n",
    "            binary_output[i] = (output == i).astype(np.uint8)\n",
    "        return binary_output\n",
    "\n",
    "    def output_formatter(\n",
    "        self,\n",
    "        model_out: np.ndarray,\n",
    "        images: Sequence[AiImage],\n",
    "        categories: Sequence[Dict[str, Any]],\n",
    "        **kwargs\n",
    "    ) -> AiCOCOImageOutputDataModel:\n",
    "\n",
    "        output = self.formatter(model_out=model_out, images=images, categories=categories)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration with InferAPP\n",
    "We could integrate our defined inference model with FLaVor `InferAPP`, a FastAPI application. To initiate the application, users have to define `input_data_model` and `output_data_model` which are the standard input and output structure for the service. Then, provide `infer_function` as the main inference operation. After initiate the service, `/invocations` API end point would be available to process the inference request. We encourge users to implement a stand-alone python script based on this jupyter notebook tutorial."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) to initiate application in jupyter notebook, you have to run the following block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block is only for jupyter notebook. You don't need this in stand-alone script.\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = InferAPP(\n",
    "    infer_function=SegmentationInferenceModel(),\n",
    "    input_data_model=AiCOCOImageInputDataModel,\n",
    "    output_data_model=AiCOCOImageOutputDataModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.run(port=int(os.getenv(\"PORT\", 9111)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send request\n",
    "We can send request to the running server by `send_request.py` which opens the input files and the coresponding JSON file and would be sent via formdata. We expect to have response in AiCOCO format.\n",
    "\n",
    "#### retrieve testing data\n",
    "```bash\n",
    "# pwd: examples/inference\n",
    "wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1h23vhCuUIKJkFw6jC7VV2XU9lGFuxrLw' -O test_data/seg/img0062.zip && mkdir test_data/seg/img0062 && unzip test_data/seg/img0062.zip -d test_data/seg/img0062\n",
    "```\n",
    "\n",
    "```bash\n",
    "# pwd: examples/inference\n",
    "python send_request.py -f \"test_data/seg/img0062/*.dcm\" -d test_data/seg/input_3d_dcm.json\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dockerfile\n",
    "In order to interact with other services, we have to wrap the inference model into a docker container. Here's an example of the dockerfile."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```dockerfile\n",
    "FROM nvidia/cuda:12.2.2-runtime-ubuntu20.04\n",
    "\n",
    "RUN apt-get update \\\n",
    "    && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \\\n",
    "        python3 \\\n",
    "        python3-pip \\\n",
    "    && ln -sf /usr/bin/python3 /usr/bin/python\n",
    "    && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends wget\\\n",
    "\n",
    "RUN pip install torch==2.1.0+cu121 --extra-index-url https://download.pytorch.org/whl/cu121 --default-timeout=1000\n",
    "RUN pip install https://github.com/ailabstw/FLaVor/archive/refs/heads/release/stable.zip\n",
    "RUN pip install monai==1.1.0 && pip install \"monai[einops]\"\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "RUN wget https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/swin_unetr.tiny_5000ep_f12_lr2e-4_pretrained.pt /app/\n",
    "\n",
    "COPY your_script.py  /app/\n",
    "\n",
    "CMD [\"python\", \"your_script.py\"]\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flavor-ObFXzz_m-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
