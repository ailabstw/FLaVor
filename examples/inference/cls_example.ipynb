{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D classification task with FLaVor inference service"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide will walk you through tailoring the FLaVor inference service for 2D classification tasks using the model from [cft-chexpert](https://github.com/maxium0526/cft-chexpert)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the working environment, please ensure you have the following dependencies installed:\n",
    "\n",
    "```txt\n",
    "python > 3.8\n",
    "torch > 1.13\n",
    "```\n",
    "\n",
    "or simply run:\n",
    "\n",
    "```sh\n",
    "poetry install --with examples --extras infer \n",
    "```\n",
    "\n",
    "Next, clone [cft-chexpert](https://github.com/maxium0526/cft-chexpert).\n",
    "```sh\n",
    "# pwd: examples/inference\n",
    "git clone https://github.com/maxium0526/cft-chexpert.git chexpert\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from chexpert.utils.wrappers import Wrapper\n",
    "from flavor.serve.apps import InferAPP\n",
    "from flavor.serve.inference import (\n",
    "    BaseAiCOCOInferenceModel,\n",
    "    BaseAiCOCOInputDataModel,\n",
    "    BaseAiCOCOOutputDataModel,\n",
    ")\n",
    "from flavor.serve.models import AiImage, InferCategory\n",
    "from flavor.serve.strategies import AiCOCOClassificationOutputStrategy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup inference model\n",
    "\n",
    "In this section, we would create `ClassificationInferenceModel` inheriting from `BaseAiCOCOInferenceModel`. There are few abstract methods that we must override such as `define_inference_network`, `set_categories`, `set_regressions`, `data_reader` and `output_formatter`. As for `preprocess`, `inference` and `postprocess`, it is optional but here we override them since we are executing a 3D model.\n",
    "\n",
    "Firstly, we need to implement submethods: `define_inference_network`, `set_categories` and `set_regressions`. These are defined in the `__init__()` constructor of the parent class `BaseAiCOCOInferenceModel`. `define_inference_network` defines your inference network and loads its pre-trained weight. `set_categories` and `set_regressions` define category and regression information. For example, a classification output would contain `c` channels. We need to show the exact meaning of each channel by specifying in `set_categories`. Refer to the following example for more detail.\n",
    "\n",
    "Next, we implement other submethods that would be used in the `__call__` function of our inference model. See below workflow."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `__call__` function workflow for the inference model\n",
    "![__call__](images/call.png \"inference workflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ClassificationInferenceModel(BaseAiCOCOInferenceModel):\n",
    "    def __init__(self):\n",
    "        self.formatter = AiCOCOClassificationOutputStrategy()\n",
    "\n",
    "        self.thresholds = {\n",
    "            \"Atelectasis\": 0.3,\n",
    "            \"Cardiomegaly\": 0.04,\n",
    "            \"Consolidation\": 0.17,\n",
    "            \"Edema\": 0.12,\n",
    "            \"Enlarged Cardiomediastinum\": 0.09,\n",
    "            \"Fracture\": 0.07,\n",
    "            \"Lung Lesion\": 0.05,\n",
    "            \"Lung Opacity\": 0.26,\n",
    "            \"No Finding\": 0.06,\n",
    "            \"Pleural Effusion\": 0.14,\n",
    "            \"Pleural Other\": 0.02,\n",
    "            \"Pneumonia\": 0.04,\n",
    "            \"Pneumothorax\": 0.31,\n",
    "            \"Support Devices\": 0.49,\n",
    "        }\n",
    "        super().__init__()\n",
    "\n",
    "    def define_inference_network(self):\n",
    "        return Wrapper(os.path.join(os.getcwd(), \"chexpert/instances/optimized_model.h5\"))\n",
    "\n",
    "    def set_categories(self):\n",
    "        categories = [\n",
    "            {\"name\": \"Atelectasis\"},\n",
    "            {\"name\": \"Cardiomegaly\"},\n",
    "            {\"name\": \"Consolidation\"},\n",
    "            {\"name\": \"Edema\"},\n",
    "            {\"name\": \"Enlarged Cardiomediastinum\"},\n",
    "            {\"name\": \"Fracture\"},\n",
    "            {\"name\": \"Lung Lesion\"},\n",
    "            {\"name\": \"Lung Opacity\"},\n",
    "            {\"name\": \"No Finding\"},\n",
    "            {\"name\": \"Pleural Effusion\"},\n",
    "            {\"name\": \"Pleural Other\"},\n",
    "            {\"name\": \"Pneumonia\"},\n",
    "            {\"name\": \"Pneumothorax\"},\n",
    "            {\"name\": \"Support Devices\"},\n",
    "        ]\n",
    "        return categories\n",
    "\n",
    "    def set_regressions(self):\n",
    "        return None\n",
    "\n",
    "    def data_reader(self, files: Sequence[str], **kwargs) -> Tuple[np.ndarray, None, None]:\n",
    "        img = cv2.imread(files[0], cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "        img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_AREA)\n",
    "        img = img.astype(np.float32)\n",
    "\n",
    "        return img, None, None\n",
    "\n",
    "    def inference(self, x: np.ndarray) -> np.ndarray:\n",
    "        return self.network.predict(x)\n",
    "\n",
    "    def output_formatter(\n",
    "        self,\n",
    "        model_out: np.ndarray,\n",
    "        images: Sequence[AiImage],\n",
    "        categories: List[InferCategory],\n",
    "        **kwargs\n",
    "    ) -> Any:\n",
    "        format_output = np.zeros(len(categories))\n",
    "        for i, category in enumerate(categories):\n",
    "            name = category[\"name\"]\n",
    "            format_output[i] = int(model_out[name] > self.thresholds[name])\n",
    "\n",
    "        output = self.formatter(model_out=format_output, images=images, categories=categories)\n",
    "        return output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration with InferAPP\n",
    "We could integrate our defined inference model with FLaVor `InferAPP`, a FastAPI application. To initiate the application, users have to define `input_data_model` and `output_data_model` which are the standard input and output structure for the service. Then, provide `infer_function` as the main inference operation. After initiate the service, `/invocations` API end point would be available to process the inference request. We encourge users to implement a stand-alone python script based on this jupyter notebook tutorial."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) to initiate application in jupyter notebook, you have to run the following block.\n",
    "\n",
    "```python\n",
    "# This block is only for jupyter notebook. You don't need this in stand-alone script.\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = InferAPP(\n",
    "    infer_function=ClassificationInferenceModel(),\n",
    "    input_data_model=BaseAiCOCOInputDataModel,\n",
    "    output_data_model=BaseAiCOCOOutputDataModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.run(port=int(os.getenv(\"PORT\", 9111)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send request\n",
    "We can send request to the running server by `send_request.py` which opens the input files and the coresponding JSON file and would be sent via formdata. We expect to have response in AiCOCO format.\n",
    "\n",
    "```bash\n",
    "# pwd: examples/inference\n",
    "python send_request.py -f chexpert/demo_img.jpg -d test_data/cls/input.json\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dockerfile\n",
    "In order to interact with other services, we have to wrap the inference model into a docker container. Here's an example of the dockerfile."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```dockerfile\n",
    "FROM nvidia/cuda:12.2.2-runtime-ubuntu20.04\n",
    "\n",
    "RUN apt-get update \\\n",
    "    && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \\\n",
    "        python3.9 \\\n",
    "        python3-pip \\\n",
    "    && ln -sf /usr/bin/python3.9 /usr/bin/python\n",
    "    && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends wget git\\\n",
    "\n",
    "RUN pip install torch==2.1.0+cu121 --extra-index-url https://download.pytorch.org/whl/cu121 --default-timeout=1000\n",
    "RUN pip install https://github.com/ailabstw/FLaVor/archive/refs/heads/release/stable.zip -U && pip install \"flavor[infer]\"\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "RUN git clone https://github.com/maxium0526/cft-chexpert.git /app/chexpert\n",
    "\n",
    "COPY your_script.py  /app/\n",
    "\n",
    "CMD [\"python\", \"your_script.py\"]\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flavor-ObFXzz_m-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
