{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Segmentation Task with Triton Inference Service\n",
    "\n",
    "This guide will walk you through tailoring the FLaVor inference service for 2D toy segmentation tasks using Triton Inference Server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "Ensure you have the following dependencies installed:\n",
    "\n",
    "```\n",
    "python >= 3.8\n",
    "```\n",
    "or simply run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry install --with triton_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, Dict, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from flavor.serve.apps import InferAPP\n",
    "from flavor.serve.inference import (\n",
    "    BaseAiCOCOImageInferenceModel,\n",
    "    BaseAiCOCOImageInputDataModel,\n",
    "    BaseAiCOCOImageOutputDataModel,\n",
    ")\n",
    "from flavor.serve.models import AiImage, InferCategory\n",
    "from flavor.serve.strategies import AiCOCOSegmentationOutputStrategy\n",
    "\n",
    "from flavor.serve.inference import TritonInferenceModel, TritonInferenceModelSharedSystemMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup inference model\n",
    "\n",
    "To perform inference with models on Triton Inference Server, you can specify the network names, versions, and the remote host URL by using the `TritonInferenceModel` class. For a more memory-efficient setup, particularly if you are hosting the Triton Inference Server on the same machine, you can use the `TritonInferenceModelSharedSystemMemory` class.\n",
    "\n",
    "To dive in more about the implementation, we would create `SegmentationTritonInferenceModel` inheriting from `BaseAiCOCOImageInferenceModel`. There are few abstract methods that we must override such as `define_inference_network`, `set_categories`, `set_regressions`, `data_reader` and `output_formatter`. For the inference process related methods such as `preprocess`, `inference` and `postprocess`, we override them if necessary. `preprocess` and `postprocess` would remain an identical operation if unmodified. `inference` by default runs `self.forward(x)`.\n",
    "\n",
    "Firstly, we need to implement submethods: `define_inference_network`, `set_categories` and `set_regressions`. These are defined in the `__init__()` constructor of the parent class `BaseAiCOCOImageInferenceModel`. `define_inference_network` defines your inference network and loads its pre-trained weight. Here, we call `TritonInferenceModel` or `TritonInferenceModelSharedSystemMemory` depending on the setting, and provide a few arguments to specify the model we want to use. `set_categories` and `set_regressions` define category and regression information. For example, a segmentation output would contain `c` channels. We need to show the exact meaning of each channel by specifying in `set_categories`. Refer to the following example for more detail.\n",
    "\n",
    "Next, we implement other submethods that would be used in the `__call__` function of our inference model. See below workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `__call__` function workflow for the inference model\n",
    "![__call__](images/call.png \"inference workflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationTritonInferenceModel(BaseAiCOCOImageInferenceModel):\n",
    "    def __init__(self, triton_url: str = \"triton:8000\", model_name: str = \"toyseg\", model_version: str = \"\", is_shared_memory: bool = False):\n",
    "        self.formatter = AiCOCOSegmentationOutputStrategy()\n",
    "\n",
    "        self.triton_url = triton_url\n",
    "        self.model_name = model_name\n",
    "        self.model_version = model_version\n",
    "        self.is_shared_memory = is_shared_memory\n",
    "        super().__init__()\n",
    "\n",
    "    def define_inference_network(self):\n",
    "        if self.is_shared_memory:\n",
    "            return TritonInferenceModelSharedSystemMemory(\n",
    "                self.triton_url, self.model_name, self.model_version\n",
    "            )\n",
    "        else:\n",
    "            return TritonInferenceModel(self.triton_url, self.model_name, self.model_version)\n",
    "\n",
    "    def set_categories(self):\n",
    "        categories = [\n",
    "            {\"name\": \"Background\", \"display\": False},\n",
    "            {\"name\": \"Foreground 1\", \"display\": True},\n",
    "            {\"name\": \"Foreground 2\", \"display\": True},\n",
    "        ]\n",
    "        return categories\n",
    "\n",
    "    def set_regressions(self):\n",
    "        return None\n",
    "\n",
    "    def data_reader(self, files: Sequence[str], **kwargs) -> Tuple[np.ndarray, None, None]:\n",
    "        img = cv2.imread(files[0])\n",
    "        return img, None, None\n",
    "\n",
    "    def preprocess(self, data: np.ndarray) -> np.ndarray:\n",
    "        data = cv2.resize(data, (256, 256), interpolation=cv2.INTER_NEAREST)\n",
    "        data = np.transpose(data, (2, 0, 1))  # h, w, c -> c, h, w\n",
    "        data = np.expand_dims(data, axis=0)  # c, h, w -> 1, c, h, w\n",
    "        return data\n",
    "\n",
    "    def inference(self, x: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "        return self.network.forward({\"input\": x})\n",
    "\n",
    "    def postprocess(self, out_dict: Any, metadata: Any = None) -> Any:\n",
    "        out = out_dict[\"logits\"][0]  # 1, c, h, w -> c, h, w\n",
    "        onehot_out = np.zeros_like(out, dtype=np.int8)\n",
    "        out = np.argmax(out, axis=0)    \n",
    "        for i in range(len(onehot_out)):\n",
    "            onehot_out[i] = (out == i)\n",
    "        return onehot_out\n",
    "\n",
    "    def output_formatter(\n",
    "        self,\n",
    "        model_out: np.ndarray,\n",
    "        images: Sequence[AiImage],\n",
    "        categories: Sequence[InferCategory],\n",
    "        **kwargs\n",
    "    ) -> Any:\n",
    "\n",
    "        output = self.formatter(model_out=model_out, images=images, categories=categories)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration with InferAPP\n",
    "We could integrate our defined inference model with FLaVor `InferAPP`, a FastAPI application. To initiate the application, users have to define `input_data_model` and `output_data_model` which are the standard input and output structure for the service. Then, provide `infer_function` as the main inference operation. After initiate the service, `/invocations` API end point would be available to process the inference request. We encourge users to implement a stand-alone python script based on this jupyter notebook tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block is only for jupyter notebook. You don't need this in stand-alone script.\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = InferAPP(\n",
    "    infer_function=SegmentationTritonInferenceModel(triton_url=\"triton.user-hannchyun-chen:8000\", model_name=\"toyseg\"),\n",
    "    input_data_model=BaseAiCOCOImageInputDataModel,\n",
    "    output_data_model=BaseAiCOCOImageOutputDataModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.run(port=int(os.getenv(\"PORT\", 9111)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send request\n",
    "We can send request to the running server by `send_request.py` which opens the input files and the coresponding JSON file and would be sent via formdata. We expect to have response in AiCOCO format.\n",
    "```bash\n",
    "# pwd: examples/inference\n",
    "python send_request.py -f test_data/seg/300.png -d test_data/seg/input_seg.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dockerfile\n",
    "In order to interact with other services, we have to wrap the inference model into a docker container. Here's an example of the dockerfile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```dockerfile\n",
    "FROM nvidia/cuda:12.2.2-runtime-ubuntu20.04\n",
    "\n",
    "RUN apt-get update \\\n",
    "    && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \\\n",
    "        python3 \\\n",
    "        python3-pip \\\n",
    "    && ln -sf /usr/bin/python3 /usr/bin/python\n",
    "\n",
    "RUN pip install https://github.com/ailabstw/FLaVor/archive/refs/heads/release/stable.zip -U && pip install flavor\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY your_script.py  /app/\n",
    "\n",
    "CMD [\"python\", \"your_script.py\"]\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flavor-ObFXzz_m-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
