{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Segmentation Task with Triton Inference Service\n",
    "\n",
    "This guide will walk you through tailoring the FLaVor inference service for 2D segmentation toy task using Triton Inference Server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "Ensure you have the following dependencies installed:\n",
    "\n",
    "```\n",
    "python >= 3.10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/volume/stanley-huang/FLaVor/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from flavor.serve.apps import InferAPP\n",
    "from flavor.serve.inference.data_models.api import (\n",
    "    BaseAiCOCOImageInputDataModel,\n",
    "    BaseAiCOCOImageOutputDataModel,\n",
    ")\n",
    "from flavor.serve.inference.data_models.functional import AiImage\n",
    "from flavor.serve.inference.inference_models import (\n",
    "    BaseAiCOCOImageInferenceModel,\n",
    "    TritonInferenceModel,\n",
    "    TritonInferenceModelSharedSystemMemory,\n",
    ")\n",
    "from flavor.serve.inference.strategies import AiCOCOSegmentationOutputStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Model Information on Triton Inference Server\n",
    "Firstly initiate `TritonInferenceModel` with corresponding url and model name. Then, access `model_state` attribute and you can obtain information of the deployed model on triton inference server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'toyseg',\n",
       " 'version': '1',\n",
       " 'state': 'READY',\n",
       " 'backend': 'onnxruntime',\n",
       " 'max_batch_size': 4,\n",
       " 'devices': ['KIND_GPU'],\n",
       " 'inputs': [{'name': 'input', 'data_type': 'TYPE_FP32', 'dims': [3, -1, -1]}],\n",
       " 'outputs': [{'name': 'logits',\n",
       "   'data_type': 'TYPE_FP32',\n",
       "   'dims': [-1, -1, -1]}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_model = TritonInferenceModel(\n",
    "    triton_url=\"triton:8000\", model_name=\"toyseg\", model_version=\"\",\n",
    ")\n",
    "triton_model.model_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup inference model\n",
    "\n",
    "To perform inference with models on Triton Inference Server, you can specify the network names, versions, and the remote host URL by using the `TritonInferenceModel` class. For a more memory-efficient setup, particularly if you are hosting the Triton Inference Server on the same machine, you can use the `TritonInferenceModelSharedSystemMemory` class.\n",
    "\n",
    "To dive in more about the implementation, we would create `SegmentationTritonInferenceModel` inheriting from `BaseAiCOCOImageInferenceModel`. There are few abstract methods that we must override such as `define_inference_network`, `set_categories`, `set_regressions`, `data_reader` and `output_formatter`. For the inference process related methods such as `preprocess`, `inference` and `postprocess`, we override them if necessary. Here in this example, the input to the model on triton inference server must depend on the model information. Referring to the above block, we know that the input should be a dictionary with key of `input` and corresponding value in type of `TYPE_FP32` and dimension of `[3, -1, -1]` where -1 means any number. \n",
    "\n",
    "Next, we need to implement submethods: `define_inference_network`, `set_categories` and `set_regressions`. These are defined in the `__init__()` constructor of the parent class `BaseAiCOCOImageInferenceModel`. `define_inference_network` defines your inference network and loads its pre-trained weight. Here, we call `TritonInferenceModel` or `TritonInferenceModelSharedSystemMemory` depending on the setting, and provide a few arguments to specify the model we want to use. `set_categories` and `set_regressions` define category and regression information. For example, a segmentation output would contain `c` channels. We need to show the exact meaning of each channel by specifying in `set_categories`. Refer to the following example for more detail.\n",
    "\n",
    "Next, we implement other submethods that would be used in the `__call__` function of our inference model. See below workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `__call__` function workflow for the inference model\n",
    "![__call__](images/call.png \"inference workflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationTritonInferenceModel(BaseAiCOCOImageInferenceModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        triton_url: str = \"triton:8000\",\n",
    "        model_name: str = \"toyseg\",\n",
    "        model_version: str = \"\",\n",
    "        is_shared_memory: bool = False,\n",
    "    ):\n",
    "        self.formatter = AiCOCOSegmentationOutputStrategy()\n",
    "\n",
    "        self.triton_url = triton_url\n",
    "        self.model_name = model_name\n",
    "        self.model_version = model_version\n",
    "        self.is_shared_memory = is_shared_memory\n",
    "        super().__init__()\n",
    "\n",
    "    def define_inference_network(self) -> Callable:\n",
    "        if self.is_shared_memory:\n",
    "            return TritonInferenceModelSharedSystemMemory(\n",
    "                self.triton_url, self.model_name, self.model_version\n",
    "            )\n",
    "        else:\n",
    "            return TritonInferenceModel(self.triton_url, self.model_name, self.model_version)\n",
    "\n",
    "    def set_categories(self) -> List[Dict[str, Any]]:\n",
    "        categories = [\n",
    "            {\"name\": \"Background\", \"display\": False},\n",
    "            {\"name\": \"Foreground 1\", \"display\": True},\n",
    "            {\"name\": \"Foreground 2\", \"display\": True},\n",
    "        ]\n",
    "        return categories\n",
    "\n",
    "    def set_regressions(self) -> None:\n",
    "        return None\n",
    "\n",
    "    def data_reader(self, files: Sequence[str], **kwargs) -> Tuple[np.ndarray, None, None]:\n",
    "        img = cv2.imread(files[0])\n",
    "        return img, None, None\n",
    "\n",
    "    def preprocess(self, data: np.ndarray) -> np.ndarray:\n",
    "        data = cv2.resize(data, (256, 256), interpolation=cv2.INTER_NEAREST)\n",
    "        data = np.transpose(data, (2, 0, 1))  # h, w, c -> c, h, w\n",
    "        data = np.expand_dims(data, axis=0)  # c, h, w -> 1, c, h, w\n",
    "        return data\n",
    "\n",
    "    def inference(self, x: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "        return self.network.forward({\"input\": x})\n",
    "\n",
    "    def postprocess(\n",
    "        self, out_dict: Dict[str, np.ndarray], metadata: Optional[Any] = None\n",
    "    ) -> np.ndarray:\n",
    "        out = out_dict[\"logits\"][0]  # 1, c, h, w -> c, h, w\n",
    "        onehot_out = np.zeros_like(out, dtype=np.int8)\n",
    "        out = np.argmax(out, axis=0)\n",
    "        for i in range(len(onehot_out)):\n",
    "            onehot_out[i] = out == i\n",
    "        return onehot_out\n",
    "\n",
    "    def output_formatter(\n",
    "        self,\n",
    "        model_out: np.ndarray,\n",
    "        images: Sequence[AiImage],\n",
    "        categories: Sequence[Dict[str, Any]],\n",
    "        **kwargs\n",
    "    ) -> BaseAiCOCOImageOutputDataModel:\n",
    "\n",
    "        output = self.formatter(model_out=model_out, images=images, categories=categories)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration with InferAPP\n",
    "We could integrate our defined inference model with FLaVor `InferAPP`, a FastAPI application. To initiate the application, users have to define `input_data_model` and `output_data_model` which are the standard input and output structure for the service. Then, provide `infer_function` as the main inference operation. After initiate the service, `/invocations` API end point would be available to process the inference request. We encourge users to implement a stand-alone python script based on this jupyter notebook tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block is only for jupyter notebook. You don't need this in stand-alone script.\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = InferAPP(\n",
    "    infer_function=SegmentationTritonInferenceModel(triton_url=\"triton.user-hannchyun-chen:8000\", model_name=\"toyseg\"),\n",
    "    input_data_model=BaseAiCOCOImageInputDataModel,\n",
    "    output_data_model=BaseAiCOCOImageOutputDataModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.run(port=int(os.getenv(\"PORT\", 9111)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send request\n",
    "We can send request to the running server by `send_request.py` which opens the input files and the coresponding JSON file and would be sent via formdata. We expect to have response in AiCOCO format.\n",
    "```bash\n",
    "# pwd: examples/inference\n",
    "python send_request.py -f test_data/seg/300.png -d test_data/seg/input_seg.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dockerfile\n",
    "In order to interact with other services, we have to wrap the inference model into a docker container. Here's an example of the dockerfile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```dockerfile\n",
    "FROM nvidia/cuda:12.2.2-runtime-ubuntu20.04\n",
    "\n",
    "RUN apt-get update \\\n",
    "    && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \\\n",
    "        python3 \\\n",
    "        python3-pip \\\n",
    "    && ln -sf /usr/bin/python3 /usr/bin/python\n",
    "\n",
    "RUN pip install https://github.com/ailabstw/FLaVor/archive/refs/heads/release/stable.zip -U && pip install flavor\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY your_script.py  /app/\n",
    "\n",
    "CMD [\"python\", \"your_script.py\"]\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flavor-ObFXzz_m-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
